{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4ig8j3JTSaxbGWzPs/n9U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lewisnjue/AI/blob/main/knowledge_distilation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XSUDlLDOU4u7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else 'cpu'\n",
        "\n",
        "print(f\"using {device} device\")"
      ],
      "metadata": {
        "id": "qGrxNYPuVnrt",
        "outputId": "d62af62c-eee4-4ecb-f53f-7995cd31a551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the input images are grb , so they have 3 channels and are 32 x 32 ,e ach images is descrbed 3072 numbers ranging from 0 to 255. a common practice in neural networks is to normalize he input. which is done for mulipole reaasons , including avoiding sturaion in commonly used activation function and increasing numerical stability. our normalization process consists of subracting the means and dividing by he standrd deviation alogn each channel."
      ],
      "metadata": {
        "id": "S3bcB884V3rL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
        "transforms_cifar = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Loading the CIFAR-10 dataset:\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)"
      ],
      "metadata": {
        "id": "HtQ6gLnDV79Y",
        "outputId": "411a99d7-5c9a-47bc-929b-4eee047dff13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 12.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "ttduoQ2TaTsL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deeper neural network class to be used as teacher:\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Lightweight neural network class to be used as student:\n",
        "class LightNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LightNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6ResAwJtZSf9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            # inputs: A collection of batch_size images\n",
        "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
        "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "zZk7as1oZ7Tj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "nn_deep = DeepNN(num_classes=10).to(device)\n",
        "train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
        "\n",
        "# Instantiate the lightweight network:\n",
        "torch.manual_seed(42)\n",
        "nn_light = LightNN(num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "HzGElslgaEcr",
        "outputId": "1163351f-ddc2-44ca-8a1f-af8ca6a1f981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.3282010235140085\n",
            "Epoch 2/10, Loss: 0.85995360743969\n",
            "Epoch 3/10, Loss: 0.6740342608040861\n",
            "Epoch 4/10, Loss: 0.5377977293013306\n",
            "Epoch 5/10, Loss: 0.41822918742666465\n",
            "Epoch 6/10, Loss: 0.31140046282802397\n",
            "Epoch 7/10, Loss: 0.22527728426029614\n",
            "Epoch 8/10, Loss: 0.17463002896979643\n",
            "Epoch 9/10, Loss: 0.13691106591078325\n",
            "Epoch 10/10, Loss: 0.12057654050362232\n",
            "Test Accuracy: 75.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "new_nn_light = LightNN(num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "pDX7eMX9aN1d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the norm of the first layer of the initial lightweight model\n",
        "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
        "# Print the norm of the first layer of the new lightweight model\n",
        "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())"
      ],
      "metadata": {
        "id": "rH_27Z3sbDbX",
        "outputId": "31a35a83-318a-4066-c073-01118a3b6373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm of 1st layer of nn_light: 2.327361822128296\n",
            "Norm of 1st layer of new_nn_light: 2.327361822128296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
        "print(f\"DeepNN parameters: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")"
      ],
      "metadata": {
        "id": "aslwC9p-bHg1",
        "outputId": "1b19199e-92d8-47b3-aed5-52c7fef2d7fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepNN parameters: 1,186,986\n",
            "LightNN parameters: 267,738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_light_ce = test(nn_light, test_loader, device)"
      ],
      "metadata": {
        "id": "cpkLWgCZbTtB",
        "outputId": "c393f887-579a-4da8-a735-a464e5cc07b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.4709399301377708\n",
            "Epoch 2/10, Loss: 1.1618167847928489\n",
            "Epoch 3/10, Loss: 1.030767710312553\n",
            "Epoch 4/10, Loss: 0.926373452939036\n",
            "Epoch 5/10, Loss: 0.8455027415014594\n",
            "Epoch 6/10, Loss: 0.7781399855833224\n",
            "Epoch 7/10, Loss: 0.7102214229076415\n",
            "Epoch 8/10, Loss: 0.6532236542695623\n",
            "Epoch 9/10, Loss: 0.6001846083747152\n",
            "Epoch 10/10, Loss: 0.5505389866164273\n",
            "Test Accuracy: 71.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**teh big picture**\n",
        "\n",
        "you have:\n",
        "- a teacher model: big , accurate and already trained\n",
        "- a student model : smaller , less accurate (yet) , but faster adn lighter.\n",
        "\n",
        "the goal is to train the student so that it learns not only from the ground truth lables( like usual cross entropy loss) but also from the teacher ouput, wich containins  richer information.\n",
        "\n",
        "**why bother learning fomr the teacher ouputs?**\n",
        "because:\n",
        "- a well trined teacher doest just ouput it a truct with 99 centrary\n",
        "- it also tellyou , `i kinda see it also similar to an automobile , and maybe a litle bit like an airplaine`\n",
        "\n",
        "these subtle probability distributions reflex relationships between classes. the student can learn from these relationships, not just from hard ground truth lables.\n",
        "\n",
        "\n",
        "**the problem with cross-entropy alone**\n",
        "\n",
        "regular cross-entropy loss lokks at one label. it hard(eg this is truact, 100% centrainty)\n",
        "it ignores the naunced probabilites the teacher can offer.\n",
        "but ifyou look at the teacher entire softmax output, you can get soft lables like\n",
        "```makefile\n",
        "truck: 0.7\n",
        "automobile: 0.2\n",
        "airplane: 0.1\n",
        "dog: 0.0\n",
        "```\n",
        "this says more about how teeacher understands the world\n",
        "\n",
        "\n",
        "**how knowledge distillaiotn works**\n",
        "\n",
        "you combine two losses:\n",
        "1. cross-entropy loss(CE) with the real labels ( like usual classification)\n",
        "2. distillation loss: comparing the studnet ouptu to the teacher soft ouputs.\n",
        "\n",
        "\n",
        "the distillation loss helps the student mimic the teacher probability distribution , not jsut match the hard labels.\n",
        "\n",
        "\n",
        "**what do these terms mean?**\n",
        "\n",
        "1. *Temperature(T)*\n",
        "-this is a parameter used to smooth the sofmax ouput from both teaacher and student.\n",
        "- higher T -> smoother distributions-> small probability get bigger so the student pay attention to more than just the top prediction.\n",
        "example with T=1\n",
        "\n",
        "```less\n",
        "\n",
        "Teacehr softmax:[0.95,0.04,0.01]\n",
        "\n",
        "```\n",
        "with T=5\n",
        "\n",
        "```less\n",
        "Teacher softmax: [0.5,0.04,0.01]\n",
        "\n",
        "```\n",
        "\n",
        "smoother -> easier for the student to learn relative similarites between classes.\n",
        "\n",
        "\n",
        "2. soft_target_less_weight\n",
        "\n",
        "- this is the weight you assing to the distribution loss .\n",
        "- it controls how much the student focuses on mimicking the teacher soft ouputs.\n",
        "\n",
        "3. ce_loss_weight\n",
        "\n",
        "- this is teh weight for the normal cross-entropy loss with the true lables.\n",
        "- it controls how much the student focuses on learning the real lables.\n",
        "\n",
        "\n",
        "**what the pint of balancing these?**\n",
        "\n",
        "you tuen soft_target_loss_weight and ce_loss_wight dependin on:\n",
        "- how much you trust the teacher.\n",
        "- how much you still want to respect the real lables.\n",
        "\n",
        "you ca think of it like:\n",
        "\n",
        "\n",
        "`Total Loss = ce_loss_weight * CrossEntropyLoss + soft_target_loss_weight * DistillationLoss`\n",
        "\n"
      ],
      "metadata": {
        "id": "TP6TxK0RbYM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            #Soften the student logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
        "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
        "\n",
        "# Compare the student test accuracy with and without the teacher, after distillation\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
      ],
      "metadata": {
        "id": "TXACXpu_f8C_",
        "outputId": "cbdbcaa7-b493-4b63-c450-bb0d30842e41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 2.3757467836980015\n",
            "Epoch 2/10, Loss: 1.8585280073268333\n",
            "Epoch 3/10, Loss: 1.6373781712768634\n",
            "Epoch 4/10, Loss: 1.4800155147567124\n",
            "Epoch 5/10, Loss: 1.3542415176511116\n",
            "Epoch 6/10, Loss: 1.2409376774907417\n",
            "Epoch 7/10, Loss: 1.1423496724394582\n",
            "Epoch 8/10, Loss: 1.057148908715114\n",
            "Epoch 9/10, Loss: 0.9775785663548637\n",
            "Epoch 10/10, Loss: 0.9119357076447333\n",
            "Test Accuracy: 70.94%\n",
            "Teacher accuracy: 75.48%\n",
            "Student accuracy without teacher: 71.08%\n",
            "Student accuracy with CE + KD: 70.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RD3glgpxf_aa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}